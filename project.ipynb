{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tW9DgxUhE9Lx"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit pyngrok scikit-learn seaborn joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYY81jp9FX1r",
        "outputId": "3f849921-42a7-4ba3-c7a3-1d0af16a212a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"36nIAv5i870cFQbZXOps4LDxs91_61KKwf9Cypzx2R5bpnGJq\")"
      ],
      "metadata": {
        "id": "wpLntKHxFxzw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/heart_disease_uci.csv\")\n",
        "\n",
        "print(f\"Sample dataset created with {len(df)} records\")\n",
        "print(f\"Disease distribution: {df['num'].value_counts().to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27o0gaWkFSpu",
        "outputId": "e811c1ee-1809-4f80-ed8b-487d5714673c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dataset created with 920 records\n",
            "Disease distribution: {0: 411, 1: 265, 2: 109, 3: 107, 4: 28}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the app.py file\n",
        "app_code = '''import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "from io import BytesIO\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "#loading data\n",
        "def load_data(uploaded_file=None, path=None):\n",
        "    if uploaded_file is not None:\n",
        "        return pd.read_csv(uploaded_file)\n",
        "    elif path and os.path.exists(path):\n",
        "        return pd.read_csv(path)\n",
        "    return None\n",
        "\n",
        "#preprocessing\n",
        "def preprocess(df, target_col=\"num\"):\n",
        "    df = df.copy()\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    num_cols = df.select_dtypes(include=np.number).columns\n",
        "    for c in num_cols:\n",
        "        df[c] = df[c].fillna(df[c].median())\n",
        "    if target_col in df.columns:\n",
        "        if df[target_col].nunique() > 2:\n",
        "            df[target_col] = df[target_col].apply(lambda x: 1 if x > 0 else 0)\n",
        "    return df\n",
        "\n",
        "#getting features and target\n",
        "def get_features_targets(df, target_col=\"num\"):\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    return X, y\n",
        "\n",
        "#one-hot encoding\n",
        "def encode_categorical(X_train, X_test):\n",
        "    X_train_enc = pd.get_dummies(X_train)\n",
        "    X_test_enc = pd.get_dummies(X_test)\n",
        "    X_test_enc = X_test_enc.reindex(columns=X_train_enc.columns, fill_value=0)\n",
        "    return X_train_enc, X_test_enc\n",
        "\n",
        "#feature scaling\n",
        "def scale_data(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(X_train), scaler.transform(X_test), scaler\n",
        "\n",
        "#model training\n",
        "def train_model(model_name, X_train, y_train, **kwargs):\n",
        "    if model_name == \"Logistic Regression\":\n",
        "        model = LogisticRegression(max_iter=1000, **kwargs)\n",
        "    elif model_name == \"KNN\":\n",
        "        model = KNeighborsClassifier(**kwargs)\n",
        "    elif model_name == \"Random Forest\":\n",
        "        model = RandomForestClassifier(**kwargs)\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(probability=True, **kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "#evaluation\n",
        "def plot_confusion(cm, labels):\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_yticklabels(labels)\n",
        "    return fig\n",
        "\n",
        "#saving model\n",
        "def save_model_to_session(model, scaler, feature_columns):\n",
        "    st.session_state[\"trained_model\"] = model\n",
        "    st.session_state[\"scaler\"] = scaler\n",
        "    st.session_state[\"feature_columns\"] = feature_columns\n",
        "    st.session_state[\"model_trained\"] = True\n",
        "\n",
        "#download option\n",
        "def download_model():\n",
        "    model_data = {\n",
        "        \"model\": st.session_state.get(\"trained_model\"),\n",
        "        \"scaler\": st.session_state.get(\"scaler\"),\n",
        "        \"feature_columns\": st.session_state.get(\"feature_columns\")\n",
        "    }\n",
        "    buffer = BytesIO()\n",
        "    joblib.dump(model_data, buffer)\n",
        "    buffer.seek(0)\n",
        "    return buffer\n",
        "\n",
        "#app layout and stuff\n",
        "st.set_page_config(page_title=\"Heart Disease Predictor\", layout=\"wide\", page_icon=\"ðŸ«€\")\n",
        "\n",
        "st.markdown(\"<h1 style=\\\\\"color: #e74c3c; text-align: center;\\\\\">ðŸ«€ Heart Disease Prediction System</h1>\", unsafe_allow_html=True)\n",
        "st.markdown(\"### Python Project - ML-Based Diagnostic Support\")\n",
        "\n",
        "if \"model_trained\" not in st.session_state:\n",
        "    st.session_state[\"model_trained\"] = False\n",
        "\n",
        "st.sidebar.title(\"â£ Configuration\")\n",
        "st.sidebar.header(\"1ï¸. Load Dataset\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload CSV dataset\", type=[\"csv\"])\n",
        "\n",
        "df = None\n",
        "if uploaded_file:\n",
        "    df = load_data(uploaded_file=uploaded_file)\n",
        "elif os.path.exists(\"heart_disease_uci.csv\"):\n",
        "    df = load_data(path=\"heart_disease_uci.csv\")\n",
        "    st.sidebar.info(\"Using local dataset\")\n",
        "\n",
        "if df is None:\n",
        "    st.warning(\"â–· Please upload a dataset to get started!\")\n",
        "    st.stop()\n",
        "\n",
        "target_col = st.sidebar.text_input(\"Target column:\", value=\"num\")\n",
        "\n",
        "try:\n",
        "    df = preprocess(df, target_col=target_col)\n",
        "    st.sidebar.success(f\"{df.shape[0]} rows Ã— {df.shape[1]} cols\")\n",
        "except Exception as e:\n",
        "    st.sidebar.error(f\"Error: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ— Data\", \"âŒ¯âŒ² Training\", \"ð–¡Š Predict\", \"â“˜ About\"])\n",
        "\n",
        "with tab1:\n",
        "    st.header(\"Dataset Overview\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    col1.metric(\"Samples\", df.shape[0])\n",
        "    col2.metric(\"Features\", df.shape[1] - 1)\n",
        "    col3.metric(\"Classes\", df[target_col].nunique())\n",
        "    st.dataframe(df.head(10), use_container_width=True)\n",
        "\n",
        "    col_left, col_right = st.columns(2)\n",
        "    with col_left:\n",
        "        st.subheader(\"Summary Stats\")\n",
        "        st.dataframe(df.describe())\n",
        "    with col_right:\n",
        "        st.subheader(\"Target Distribution\")\n",
        "        target_counts = df[target_col].value_counts()\n",
        "        st.write(target_counts)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        target_counts.plot(kind=\"pie\", autopct=\"%1.1f%%\", labels=[\"No Disease\", \"Disease\"], colors=[\"#2ecc71\", \"#e74c3c\"], explode=[0.05, 0], ax=ax)\n",
        "        ax.set_ylabel(\"\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    if st.checkbox(\"Show Correlation Heatmap\"):\n",
        "        numeric_df = df.select_dtypes(include=[np.number])\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0, ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "with tab2:\n",
        "    st.header(\"Train ML Model\")\n",
        "    X, y = get_features_targets(df, target_col=target_col)\n",
        "    all_features = X.columns.tolist()\n",
        "    selected_features = st.multiselect(\"Select features:\", all_features, default=all_features)\n",
        "\n",
        "    if len(selected_features) == 0:\n",
        "        st.warning(\"Select at least one feature!\")\n",
        "        st.stop()\n",
        "\n",
        "    X_sel = X[selected_features]\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        test_size = st.slider(\"Test size (%):\", 10, 50, 20) / 100.0\n",
        "    with col2:\n",
        "        random_state = st.number_input(\"Random seed:\", value=42)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=test_size, random_state=int(random_state), stratify=y)\n",
        "    st.info(f\"Training: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "    X_train_enc, X_test_enc = encode_categorical(X_train, X_test)\n",
        "    X_train_scaled, X_test_scaled, scaler = scale_data(X_train_enc, X_test_enc)\n",
        "\n",
        "    model_name = st.selectbox(\"Algorithm:\", [\"Logistic Regression\", \"Random Forest\", \"KNN\", \"SVM\"])\n",
        "\n",
        "    kwargs = {}\n",
        "    if model_name == \"Logistic Regression\":\n",
        "        c = st.slider(\"C:\", 0.01, 10.0, 1.0)\n",
        "        kwargs = {\"C\": c, \"solver\": \"lbfgs\"}\n",
        "    elif model_name == \"Random Forest\":\n",
        "        n_est = st.slider(\"Trees:\", 10, 200, 100)\n",
        "        max_d = st.slider(\"Max depth (0=None):\", 0, 30, 10)\n",
        "        kwargs = {\"n_estimators\": int(n_est), \"random_state\": int(random_state)}\n",
        "        if max_d > 0:\n",
        "            kwargs[\"max_depth\"] = int(max_d)\n",
        "    elif model_name == \"KNN\":\n",
        "        n_neigh = st.slider(\"Neighbors:\", 1, 25, 5)\n",
        "        kwargs = {\"n_neighbors\": int(n_neigh)}\n",
        "    elif model_name == \"SVM\":\n",
        "        c = st.slider(\"C:\", 0.01, 10.0, 1.0)\n",
        "        kernel = st.selectbox(\"Kernel:\", [\"rbf\", \"linear\", \"poly\"])\n",
        "        kwargs = {\"C\": c, \"kernel\": kernel}\n",
        "\n",
        "    if st.button(\"âŒ¯âŒ² Train\", type=\"primary\", use_container_width=True):\n",
        "        with st.spinner(\"Training...\"):\n",
        "            try:\n",
        "                model = train_model(model_name, X_train_scaled, y_train, **kwargs)\n",
        "                save_model_to_session(model, scaler, X_train_enc.columns)\n",
        "                y_pred = model.predict(X_test_scaled)\n",
        "                acc = accuracy_score(y_test, y_pred)\n",
        "                f1 = f1_score(y_test, y_pred)\n",
        "                st.success(\"Model trained!\")\n",
        "                col1, col2 = st.columns(2)\n",
        "                col1.metric(\"Accuracy\", f\"{acc:.2%}\")\n",
        "                col2.metric(\"F1-Score\", f\"{f1:.4f}\")\n",
        "                st.text(classification_report(y_test, y_pred, target_names=[\"No Disease\", \"Disease\"]))\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                st.pyplot(plot_confusion(cm, [\"No Disease\", \"Disease\"]))\n",
        "                st.download_button(\"ðŸ¡» Download\", download_model(), \"model.joblib\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {e}\")\n",
        "\n",
        "with tab3:\n",
        "    st.header(\"Make Predictions\")\n",
        "    if not st.session_state.get(\"model_trained\"):\n",
        "        st.warning(\"Train a model first!\")\n",
        "        st.stop()\n",
        "\n",
        "    model = st.session_state[\"trained_model\"]\n",
        "    scaler = st.session_state[\"scaler\"]\n",
        "    feature_columns = st.session_state[\"feature_columns\"]\n",
        "\n",
        "    input_data = {}\n",
        "    cols = st.columns(3)\n",
        "\n",
        "    for idx, feat in enumerate(selected_features):\n",
        "        col = cols[idx % 3]\n",
        "        sample = X_sel[feat]\n",
        "        if sample.dtype == \"object\":\n",
        "            options = sorted(sample.dropna().unique().tolist())\n",
        "            input_data[feat] = col.selectbox(f\"**{feat}**\", options, key=f\"p_{feat}\")\n",
        "        else:\n",
        "            input_data[feat] = col.number_input(f\"**{feat}**\", value=float(sample.mean()), key=f\"p_{feat}\")\n",
        "\n",
        "    if st.button(\"ðŸ”Žï¸Ž Predict\", type=\"primary\"):\n",
        "        try:\n",
        "            inp_df = pd.DataFrame([input_data])[selected_features]\n",
        "            inp_df_enc = pd.get_dummies(inp_df)\n",
        "            inp_df_enc = inp_df_enc.reindex(columns=feature_columns, fill_value=0)\n",
        "            inp_scaled = scaler.transform(inp_df_enc)\n",
        "            pred = model.predict(inp_scaled)[0]\n",
        "            pred_proba = model.predict_proba(inp_scaled)[0] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "            if pred == 1:\n",
        "                st.error(\"### âš  Heart Disease Detected\")\n",
        "            else:\n",
        "                st.success(\"### â›‰ No Heart Disease\")\n",
        "\n",
        "            if pred_proba is not None:\n",
        "                col1, col2 = st.columns(2)\n",
        "                col1.metric(\"No Disease\", f\"{pred_proba[0]:.1%}\")\n",
        "                col2.metric(\"Disease\", f\"{pred_proba[1]:.1%}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "with tab4:\n",
        "    st.header(\"About\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### Heart Disease Prediction System\n",
        "\n",
        "    **Features:**\n",
        "    - Multiple ML algorithms\n",
        "    - Interactive visualizations\n",
        "    - Real-time predictions\n",
        "    - Model download\n",
        "\n",
        "    **Tech Stack:**\n",
        "    Streamlit, Scikit-learn, Pandas, NumPy\n",
        "    \"\"\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<p style='text-align: center;'>Heart Disease Prediction | Python Project</p>\", unsafe_allow_html=True)\n",
        "'''\n",
        "\n",
        "# Write the file\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"app.py file created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVlBcyaKSJVZ",
        "outputId": "7d1bfc31-9b99-41dc-f6fe-74d77b7ab193"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Clean up\n",
        "!pkill -f streamlit\n",
        "time.sleep(2)\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"APP IS LIVE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nURL: {public_url}\")\n",
        "print(\"\\nKeep this cell running!\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Launch app\n",
        "!streamlit run app.py --server.port 8501 --server.headless true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdAPnP1tSL2i",
        "outputId": "9d87bbc4-b61d-4e02-c89e-58fe0143cd96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "APP IS LIVE!\n",
            "======================================================================\n",
            "\n",
            "URL: NgrokTunnel: \"https://unbeneficed-bicompact-eleni.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "Keep this cell running!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.145.223.23:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-12-13 17:01:09.685 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-13 17:01:27.715 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-13 17:01:37.057 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-13 17:01:39.295 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-13 17:01:45.341 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}